{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f963eec4-9530-4537-a643-dfc952be3589",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Defined Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1bd375d-e8f3-4588-8e7f-cae1ab470595",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weekly_sched_extract(username, password):\n",
    "    # schedule extractor\n",
    "    import re\n",
    "    import time\n",
    "    from selenium import webdriver\n",
    "    from selenium.webdriver.common.by import By\n",
    "    from bs4 import BeautifulSoup\n",
    "    import pandas as pd\n",
    "    \n",
    "    driver = webdriver.Chrome()\n",
    "    link = r\"https://aisis.ateneo.edu/j_aisis/J_VMCS.do\"\n",
    "    \n",
    "    driver.get(link)\n",
    "    time.sleep(1)\n",
    "    \n",
    "    driver.find_element(By.NAME, 'userName').send_keys(username) \n",
    "    driver.find_element(By.NAME, 'password').send_keys(password)\n",
    "    driver.find_element(By.NAME, \"submit\").click()\n",
    "    time.sleep(1)\n",
    "    \n",
    "    driver.find_element(By.XPATH, '//a[contains(@href, \"J_VMCS.do\")]').click()\n",
    "    time.sleep(1.5)\n",
    "    \n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    driver.quit()\n",
    "    \n",
    "    # find the table with the schedule\n",
    "    all_tables = soup.find_all(\"table\")\n",
    "    schedule_table = None\n",
    "    for table in all_tables:\n",
    "        # check for presence of headers or specific keywords in table cells\n",
    "        if any(\"Time\" in cell.text.strip() for cell in table.find_all(\"td\")[:2]):\n",
    "            schedule_table = table\n",
    "            break\n",
    "    print('Schedule Found! Please wait for processing...')\n",
    "    \n",
    "    # extract data from the table and load it into dataframe\n",
    "    rows = schedule_table.find_all(\"tr\") # all rows are indicated by \"tr\" when printing the extracted soup\n",
    "    data = [] # initializing\n",
    "    headers = [header.text.strip() for header in rows[0].find_all(\"td\")]\n",
    "    # rows[0] accesses the first row, which for this soup object contains all the table headers 'Time', 'Mon, 'Tue', 'Wed', etc.\n",
    "    # find_all(\"td\") finds all the column elements inside this header row\n",
    "    # header.text.strip() extracts the text, removes any whitespace, and stores the text it extracts as headers for the df\n",
    "    \n",
    "    for row in rows[1:]:\n",
    "        # we skip the header row since it's already been skimmed over\n",
    "        cols = row.find_all(\"td\")\n",
    "        # finding all the column elements across all rows barring the header row\n",
    "        data.append([col.text.strip() for col in cols])\n",
    "    \n",
    "    sched_df = pd.DataFrame(data, columns=headers)\n",
    "    \n",
    "    # panda extractor\n",
    "    def panda_schedule_extractor(df):\n",
    "        schedules = []\n",
    "        days = df.columns[1:]  # excluding the column for 'Time' to store the names of the days\n",
    "    \n",
    "        for day in days:\n",
    "            current_class = None # initializing current_class and start_time as None\n",
    "            start_time = None\n",
    "            for index, row in df.iterrows(): # iterates over each row in the DataFrame\n",
    "                time = row['Time'] # stores the current time range from column 'Time'\n",
    "                class_name = row[day] # stores the class name for the current 'day'\n",
    "    \n",
    "                if class_name and class_name != current_class: # if class_name exists (not empty) and it's different from the current_class, the program reads it as a new class starting\n",
    "                    if current_class: # if there's an ongoing class, it still occupies the current time slot\n",
    "                        end_time = df.at[index-1, 'Time'].split('-')[1] #the end time of the current class is the end time of the previous time slot (hence index-1) \n",
    "                        schedules.append(f\"{current_class} {current_day} {start_time}-{end_time}\") #all this is appended to the schedule string here\n",
    "                    current_class = class_name # the current_class is updated to the new name of the new class\n",
    "                    current_day = day # current_day is updated to the day of the new class\n",
    "                    start_time = time.split('-')[0] # start time is set to the start time of the current time_slot\n",
    "                elif class_name != current_class: # this elif-if checks if an ongoing class is about to end\n",
    "                    if current_class: # if there's an ongoing class and class_name \n",
    "                        end_time = df.at[index-1, 'Time'].split('-')[1] \n",
    "                        schedules.append(f\"{current_class} {current_day} {start_time}-{end_time}\")\n",
    "                    current_class = None\n",
    "    \n",
    "            # Handle the last ongoing class of the day\n",
    "            if current_class:\n",
    "                end_time = time.split('-')[1] # end time of the last ongoing class is the end time of the current time slot\n",
    "                schedules.append(f\"{current_class} {current_day} {start_time}-{end_time}\")\n",
    "    \n",
    "        return schedules\n",
    "    \n",
    "    # Extract schedules\n",
    "    class_schedules = panda_schedule_extractor(sched_df)\n",
    "    \n",
    "    #print(class_schedules)\n",
    "    \n",
    "    final_sched_df = pd.DataFrame(columns = ['Subject', 'Location', 'Day', 'Start Time', 'End Time'])\n",
    "       \n",
    "    def parse_schedule(schedule):\n",
    "        # Split by spaces and parentheses\n",
    "        parts = item.split()\n",
    "        \n",
    "        # Extract subject and location\n",
    "        subject = ' '.join(parts[0:2])\n",
    "        location = ' '.join(parts[2:-2])\n",
    "        \n",
    "        # Extract day and time\n",
    "        day = parts[-2]\n",
    "        start_time_str, end_time_str = parts[-1].split('-')\n",
    "        \n",
    "        # Format times\n",
    "        start_time = f\"{start_time_str[:-2]}:{start_time_str[-2:]}\"\n",
    "        end_time = f\"{end_time_str[:-2]}:{end_time_str[-2:]}\"\n",
    "    \n",
    "        temp_list = [subject, location, day, start_time, end_time]\n",
    "    \n",
    "        return temp_list\n",
    "    \n",
    "    for item in class_schedules:\n",
    "        final_sched_df.loc[len(final_sched_df.index)] = parse_schedule(item)\n",
    "    \n",
    "    def section_specify(subject):\n",
    "        section = subject[-1] \n",
    "        final_subject = subject[0:-1]\n",
    "        final_subject += f' Section {section}'\n",
    "        return final_subject\n",
    "    \n",
    "    final_sched_df['Subject'] = final_sched_df['Subject'].apply(section_specify)\n",
    "\n",
    "    print('Uploading Table to Google Spreadsheets Now!')\n",
    "    \n",
    "    return final_sched_df\n",
    "\n",
    "'================================================='\n",
    "\n",
    "def word_list_creation(text):\n",
    "    # Transformation + Removal of Unwanted Punctuations\n",
    "    text = text.upper()\n",
    "    text = text.replace(\"\\n\",\" \")\n",
    "    \n",
    "    punctuations = r\"&$@[].,'#()-\\\"!?’_;:/●\"\n",
    "    text = text.replace('\"', \" \")\n",
    "    for symbol in punctuations:\n",
    "        text = text.replace(symbol, \" \")\n",
    "    \n",
    "    word_list = text.split()\n",
    "    \n",
    "    return word_list\n",
    "\n",
    "\"==================================================\"\n",
    "\n",
    "def keyword_find(text):\n",
    "    import spacy\n",
    "    from nltk.corpus import wordnet as wn\n",
    "    from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "    \n",
    "    # Loading spaCy model with word vectors, disabling unnecessary components 'parser', 'ner', and 'textcat' for max efficiency\n",
    "    nlp = spacy.load('en_core_web_md', disable=['parser', 'ner', 'textcat'])\n",
    "    \n",
    "    # Given seed terms\n",
    "    seed_terms = [\"TEST\", \"EXAM\", \"PAPER\", \"ESSAY\", \"PROJECT\", \"PODCAST\", \"CASE\", \"ASSESSMENT\", \"PROBLEM SET\", \"NEWSLETTER\", \"ANALYSIS\", \"ASSIGNMENT\", \"SUMMATIVE\"]\n",
    "    \n",
    "    # Custom stop words - these are words we want the program to skip over, no need to analyze these since they're unrelated to our seed terms\n",
    "    custom_stop_words = set([\n",
    "        'course', 'professor', 'syllabus', 'university', 'student', 'class', \n",
    "        'week', 'lecture', 'grade', 'school', 'core', 'curriculum', 'outcomes', 'cclo', 'critical', 'program', 'plo', 'outline', 'hours',\n",
    "        'module', 'print', 'online', 'onsite', 'instructor/s', 'information', 'year', 'department', 'instructor', \n",
    "    ])\n",
    "    \n",
    "    keyword = \"LEARNING METHODS\"\n",
    "    def preprocess_syllabus(syllabus_text, keyword):\n",
    "        # remove all text that come after \"LEARNING METHODS\"\n",
    "        def remove_text_after_string(text, keyword):\n",
    "            keyword_upper = keyword.upper()\n",
    "            index = text.find(keyword_upper)\n",
    "            if index != -1:\n",
    "                return text[:index]\n",
    "            return text\n",
    "    \n",
    "        # Remove text after the specified keyword\n",
    "        syllabus_text = remove_text_after_string(syllabus_text, keyword)\n",
    "    \n",
    "        # transformations to remove unwanted punctuations and convert to upper case\n",
    "        punctuations = r\"&$@[].,'#()-\\\"!?’_;:/●\"\n",
    "        syllabus_text = syllabus_text.upper().replace(\"\\n\", \" \").replace('\"', \" \")\n",
    "        for symbol in punctuations:\n",
    "            syllabus_text = syllabus_text.replace(symbol, \" \")\n",
    "    \n",
    "        # tokenize, remove stopwords, and filter by part of speech (POS) tags, mainly NOUN, VERB, ADJ (adjective), and ADV (adverb)\n",
    "        doc = nlp(syllabus_text)\n",
    "        tokens = [\n",
    "            token.text for token in doc \n",
    "            if token.text.lower() not in ENGLISH_STOP_WORDS # this refers to spaCy's own database of common stop words, such as \"and\", \"or\", \"the\", etc.\n",
    "            and token.text.lower() not in custom_stop_words # this is based on our stop words\n",
    "            and token.pos_ in {\"NOUN\"} # checks if the word is tagged as a noun\n",
    "        ]\n",
    "        return tokens # if all conditions are met they're included in the list of words to be analyzed\n",
    "    \n",
    "    def calculate_similarity_wordnet(word, seed_terms, cache):\n",
    "        if word in cache: # super important to use caches to avoid redundant calculations\n",
    "            # function first checks if the word it's analyzing has already been analyzed for similarity\n",
    "            return cache[word] # if the word has already been analyzed, function immediately returns the cached value (this saves so much time)\n",
    "        \n",
    "        max_similarity = 0 # initialized to 0\n",
    "        word_synsets = wn.synsets(word) # retrieves all synsets (synonyms) for the current word being analyzed; synonyms lifted from WordNet database\n",
    "        for term in seed_terms:\n",
    "            term_synsets = wn.synsets(term) # retrieves all WordNet synonyms for each seed term in the seed_term list\n",
    "            for ws in word_synsets:\n",
    "                for ts in term_synsets:\n",
    "                    similarity = wn.wup_similarity(ws, ts) # compares the similarity between the synsets of one word with the synsets of one seed_term\n",
    "                    if similarity and similarity > max_similarity: # if the similarity is significant, it's recorded as the max_similarity\n",
    "                        max_similarity = similarity \n",
    "        cache[word] = max_similarity\n",
    "        return max_similarity\n",
    "    \n",
    "    def calculate_similarity_vectors(words, seed_terms, nlp, cache): # same logic as the previous function, except this time, it's based on vectors\n",
    "        words_to_process = [word for word in words if word not in cache]\n",
    "        word_docs = nlp(\" \".join(words_to_process))\n",
    "        term_docs = nlp(\" \".join(seed_terms)) # being very honest i have 0 idea how this works, i just read the documentation for it, and with my trusty \n",
    "        # steed chatgpt I cooked this up and it works\n",
    "        # supposedly this is used in conjunction with WN-similarity for more accurate results: when I tried to use only one without the other, the results\n",
    "        # were indeed less accurate and significantly far from the seed terms\n",
    "        \n",
    "        for word_doc in word_docs:\n",
    "            if not word_doc.has_vector:\n",
    "                cache[word_doc.text] = 0  # Assign a default similarity of 0 for words without vectors\n",
    "                continue  \n",
    "            max_similarity = 0\n",
    "            for term_doc in term_docs:\n",
    "                if not term_doc.has_vector:\n",
    "                    continue  \n",
    "                similarity = word_doc.similarity(term_doc)\n",
    "                if similarity > max_similarity:\n",
    "                    max_similarity = similarity\n",
    "                    \n",
    "            cache[word_doc.text] = max_similarity\n",
    "            \n",
    "        return [cache[word] for word in words if word in cache]\n",
    "\n",
    "    def extract_keywords(syllabus_text, num_keywords=10):\n",
    "        # Preprocess the syllabus text\n",
    "        preprocessed_tokens = preprocess_syllabus(syllabus_text, keyword)\n",
    "        \n",
    "        wordnet_cache = {}\n",
    "        vector_cache = {}\n",
    "        \n",
    "        # Calculate combined similarity for each token\n",
    "        token_similarities = {}\n",
    "        wordnet_similarities = [calculate_similarity_wordnet(token, seed_terms, wordnet_cache) for token in preprocessed_tokens]\n",
    "        vector_similarities = calculate_similarity_vectors(preprocessed_tokens, seed_terms, nlp, vector_cache)\n",
    "        \n",
    "        for token, wn_sim, vec_sim in zip(preprocessed_tokens, wordnet_similarities, vector_similarities):\n",
    "            combined_similarity = (wn_sim + vec_sim) / 2  # Average of both similarities\n",
    "            token_similarities[token] = combined_similarity\n",
    "        \n",
    "        # Sort tokens by combined similarity\n",
    "        sorted_tokens = sorted(token_similarities.items(), key=lambda item: item[1], reverse=True)\n",
    "        \n",
    "        # Extract top keywords based on combined similarity\n",
    "        keywords = [token for token, sim in sorted_tokens[:num_keywords]]\n",
    "        \n",
    "        return keywords\n",
    "    \n",
    "    keywords = extract_keywords(text)\n",
    "    seed_copy = seed_terms.copy()\n",
    "    final_keywords_set = list(set(seed_copy))\n",
    "    \n",
    "    for i in keywords:\n",
    "        if i not in final_keywords_set:\n",
    "            final_keywords_set.append(i)\n",
    "    return final_keywords_set\n",
    "\n",
    "\"==================================================\"\n",
    "\n",
    "def date_find(word_list, text):\n",
    "    # DATA CLEANER\n",
    "    # Process 1: finds the general area of the assessments in the string\n",
    "    keywords_list = keyword_find(text)\n",
    "    \n",
    "    process1_list = []\n",
    "    for word in range(len(word_list)):\n",
    "        if word_list[word] in keywords_list:\n",
    "            process1_list.append(word_list[word-1:word+15]) # the reason why i-1 is because things like TEST are preceeded by things like LONG or SUMMATIVE\n",
    "        \n",
    "    \n",
    "    # Process 2: finds the one with dates in them, the ones with a set deadline\n",
    "    months_list = [\"JAN\",\"JANUARY\",\"FEB\",\"FEBRUARY\",\"MAR\",\"MARCH\",\"APR\",\"APRIL\",\"MAY\",\"JUN\",\"JUNE\",\"JUL\",\"JULY\",\"AUG\",\"AUGUST\",\"SEP\",\"SEPT\", \"SEPTEMBER\", \"OCT\",\"OCTOBER\",\"NOV\",\"NOVEMBER\",\"DEC\",\"DECEMBER\"]\n",
    "    \n",
    "    process2_list = []\n",
    "    for process1_sublists in process1_list:\n",
    "        for process1_sublist_elements in process1_sublists:\n",
    "            if process1_sublist_elements in months_list: # checks if one of the elements in the process1_sublists has the one of the months listed in months_list\n",
    "                process2_list.append(process1_sublists)\n",
    "    \n",
    "    # Process 3: get some description followed by dates\n",
    "    process3_list = []\n",
    "    for process2_sublists in process2_list:\n",
    "        for element in range(len(process2_sublists)):\n",
    "            if process2_sublists[element].isdigit() == True and element > 2: # j > 2 to account for things like LONG TEST 2, SUMMATIVE EXAM 1\n",
    "                if process2_sublists[0:element+1][-2] in months_list:\n",
    "                    process3_list.append(process2_sublists[0:element+1]) # appends description with the digit of the date as the last item of that list (month-day format)\n",
    "                else:\n",
    "                    process3_list.append(process2_sublists[0:element+2]) # same thing as above but (day-month format)\n",
    "                    \n",
    "    \n",
    "    # Process 4: separate the descriptions list from the dates list\n",
    "    process4_list = []\n",
    "    for process3_sublists in process3_list:\n",
    "        day_month_fixer = [] # resets the list; important for the cases when multiple dates are formatted as day-month\n",
    "        process4_list.append(process3_sublists[0:-2]) # description separator\n",
    "        if process3_sublists[-2:len(process3_sublists)][-2] in months_list: # for month-day cases\n",
    "            process4_list.append(process3_sublists[-2:len(process3_sublists)])\n",
    "        else:\n",
    "            # converts the day-month into month-day\n",
    "            day_month_fixer.append(process3_sublists[-2:len(process3_sublists)][-1])\n",
    "            day_month_fixer.append(process3_sublists[-2:len(process3_sublists)][-2])\n",
    "            process4_list.append(day_month_fixer)\n",
    "    \n",
    "    # Process 5: convert both the descriptions and the dates list into strings\n",
    "    process5_list = []\n",
    "    for process4_sublists in process4_list:\n",
    "        processed_string = \"\"\n",
    "        for every_word in process4_sublists:\n",
    "            processed_string += every_word + \" \"\n",
    "        process5_list.append(processed_string.rstrip()) # rstrip to clean the space at the end\n",
    "    \n",
    "    # Process 6: final catcher to make sure that the key-value in the dictionary are description-date\n",
    "    process6_list = []\n",
    "    k = 1\n",
    "    while k <= len(process5_list):\n",
    "        if process5_list[k][0:process5_list[k].find(\" \")] in months_list: # print process5_list, there are some shady values that get through, these clears any non-month values\n",
    "            process6_list.append(process5_list[k-1])\n",
    "            process6_list.append(process5_list[k])\n",
    "        k += 2\n",
    "    \n",
    "    # Process 7: return a dictionary with the description as the key and the dates as the value\n",
    "    final_dict = {}\n",
    "    for l in range(0, len(process6_list),2):\n",
    "        final_dict[process6_list[l]] = process6_list[l+1]\n",
    "    \n",
    "    # Data Cleaning\n",
    "    remove_list = []\n",
    "    key_list = list(final_dict.keys())\n",
    "    for key in range(len(final_dict)):\n",
    "        if key_list[key] in key_list[key-1]:\n",
    "            remove_list.append(key_list[key])        \n",
    "    \n",
    "    for item in remove_list:\n",
    "        final_dict.pop(item)\n",
    "\n",
    "    return final_dict\n",
    "\n",
    "\"==================================================\"\n",
    "\n",
    "def panda_creation(dict, word_list, df):\n",
    "    import pandas as pd\n",
    "\n",
    "    def find_words_index(word_list, word1, word2):\n",
    "        for i in range(len(word_list) - 1):\n",
    "            if word_list[i].lower() == word1.lower() and word_list[i + 1].lower() == word2.lower():\n",
    "                return i+1\n",
    "                break\n",
    "    \n",
    "    def title_find(word_list):\n",
    "        num1 = find_words_index(word_list, 'COURSE', 'NUMBER')\n",
    "        num2 = find_words_index(word_list, 'NO', 'OF')\n",
    "    \n",
    "        if type(num1) == int and type(num2) == int:\n",
    "            title_list = word_list[num1+1:num2-1]\n",
    "            \n",
    "            if len(title_list) == 3:\n",
    "                subject = title_list[0] + ' ' + title_list[1] + '.' + title_list[2]\n",
    "            else:\n",
    "                subject = ' '.join(title_list)\n",
    "            return subject\n",
    "        return \"MISSING SUBJECT\"\n",
    "    \n",
    "    def location_find(word_list):\n",
    "        \n",
    "        if 'VENUE' in word_list and 'SECTION' in word_list:\n",
    "            num3 = word_list.index('VENUE')\n",
    "            num4 = word_list.index('SECTION')\n",
    "            location_list = word_list[num3 + 1: num4]\n",
    "    \n",
    "            if len(location_list) > 8 or len(location_list) < 1:\n",
    "                location = 'MISSING INFORMATION'\n",
    "            else:\n",
    "                location = ' '.join(location_list)\n",
    "            return location\n",
    "        return \"MISSING LOCATION\"\n",
    "        \n",
    "    title = title_find(word_list)\n",
    "    location = location_find(word_list)\n",
    "    \n",
    "    for key in dict:\n",
    "        temp_row = [title, key, location, dict[key]]\n",
    "        df.loc[len(df.index)] = temp_row\n",
    "\n",
    "'===================================================='\n",
    "\n",
    "def create_events():\n",
    "    import os.path  # to handle file paths\n",
    "    from datetime import datetime as dt # for handling dates and times\n",
    "    import pytz  # Import the pytz library for time zone management\n",
    "    \n",
    "    # Import necessary modules for Google authentication and API access\n",
    "    from google.auth.transport.requests import Request  # For handling HTTP requests during authentication\n",
    "    from google.oauth2.credentials import Credentials  # For managing OAuth 2.0 credentials\n",
    "    from google_auth_oauthlib.flow import InstalledAppFlow  # For handling OAuth 2.0 flow for installed apps\n",
    "    from googleapiclient.discovery import build  # For creating a service object to access the Google API\n",
    "    from googleapiclient.errors import HttpError  # For handling HTTP errors from the API\n",
    "    \n",
    "    from datetime import datetime\n",
    "    \n",
    "    def parse_date(date_str):\n",
    "        # Define the year you want to use\n",
    "        year = datetime.now().year\n",
    "    \n",
    "        # Parse the date string \"JULY 13\" to \"YYYY-MM-DD\"\n",
    "        date_obj = datetime.strptime(f\"{year} {date_str}\", \"%Y %B %d\")\n",
    "        return date_obj.date().isoformat()\n",
    "        \n",
    "    Scopes = ['https://www.googleapis.com/auth/calendar']\n",
    "    \n",
    "    \n",
    "    creds = None\n",
    "    if os.path.exists('token.json'):\n",
    "        creds = Credentials.from_authorized_user_file('token.json')\n",
    "    \n",
    "    if not creds or not creds.valid:\n",
    "        if creds and creds.expired and creds.refresh_token:\n",
    "            creds.refresh(Request())\n",
    "        else:\n",
    "            flow = InstalledAppFlow.from_client_secrets_file('credentials.json', Scopes)\n",
    "            creds = flow.run_local_server(port = 0)\n",
    "    \n",
    "        with open('token.json','w') as  token:\n",
    "            token.write(creds.to_json())\n",
    "        \n",
    "    service = build(\"calendar\",'v3', credentials = creds)\n",
    "    manila_tz = pytz.timezone('Asia/Manila')\n",
    "        \n",
    "    googleSheetId = '1m8badixRPCpTt-dKThNeqW98PK4naJpHsAoUKA_0Uro'\n",
    "    workSheetName = 'Syllabus_Dates'\n",
    "    URL = 'https://docs.google.com/spreadsheets/d/{0}/gviz/tq?tqx=out:csv&sheet={1}'.format(googleSheetId, workSheetName)\n",
    "    date_df = pd.read_csv(URL)\n",
    "        \n",
    "    if date_df.empty:\n",
    "        print(\"No data found in Syllabus_Dates sheet.\")\n",
    "    else:\n",
    "        date_df['Summary'] = date_df.apply(lambda x: x.Subject + ' - ' + x.Summary, axis=1)\n",
    "        del date_df['Subject']\n",
    "        date_df['Description'] = date_df['Description'].apply(lambda x: '' if pd.isna(x) else x)\n",
    "        date_df['Color ID'] = date_df['Color ID'].apply(lambda x: 0 if pd.isna(x) else int(x.split(' - ')[0]))\n",
    "        \n",
    "        for index, row in date_df.iterrows():  \n",
    "            event = {\n",
    "                'summary': row['Summary'],\n",
    "                'location': row['Location'],\n",
    "                'description': row['Description'],\n",
    "                'colorId': row['Color ID'],\n",
    "                'start': {\n",
    "                    'date': parse_date(row['Date']),\n",
    "                    'timeZone': 'Asia/Manila'\n",
    "                },\n",
    "                'end': {\n",
    "                    'date': parse_date(row['Date']),\n",
    "                    'timeZone': 'Asia/Manila'\n",
    "                },\n",
    "                'recurrence': ['RRULE:FREQ=DAILY;COUNT=1'],\n",
    "            }\n",
    "            event = service.events().insert(calendarId='primary', body=event).execute()\n",
    "            print(f\"Event {index + 1} of {len(date_df)} created: {row['Summary']}\")\n",
    "    \n",
    "    print('')\n",
    "    \n",
    "    '===================================================='\n",
    "    \n",
    "    googleSheetId2 = '1HIzSiip1TT7wwGRrLnxLwEnqickJrbc-EunKEmY-z_k'\n",
    "    workSheetName2 = 'Weekly_Schedule'\n",
    "    URL2 = 'https://docs.google.com/spreadsheets/d/{0}/gviz/tq?tqx=out:csv&sheet={1}'.format(googleSheetId2, workSheetName2)\n",
    "    schedule_df = pd.read_csv(URL2)\n",
    "    \n",
    "    if schedule_df.empty:\n",
    "        print(\"No data found in Weekly_Schedule sheet.\")\n",
    "    else:\n",
    "        schedule_df['Color ID'] = schedule_df['Color ID'].apply(lambda x: 0 if pd.isna(x) else int(x.split(' - ')[0]))\n",
    "        schedule_df['Day'] = schedule_df['Day'].apply(lambda x: x[0:2].upper())\n",
    "        \n",
    "        def parse_datetime(year, date_str, time_str):\n",
    "            # Combine the provided date string with the time string to create a datetime object\n",
    "            datetime_obj = dt.strptime(f\"{year} {date_str} {time_str}\", \"%Y %B %d %H:%M\")\n",
    "            return datetime_obj.isoformat()\n",
    "    \n",
    "        def create_weekly_event(row, current_year):\n",
    "            start_datetime = parse_datetime(current_year, row['Start of Classes'], row['Start Time'])\n",
    "            end_datetime = parse_datetime(current_year, row['Start of Classes'], row['End Time'])\n",
    "            \n",
    "            # Convert end_date_str to the UNTIL format\n",
    "            end_date_obj = dt.strptime(row['End of Classes'], \"%B %d\")\n",
    "            until_date = end_date_obj.replace(year=current_year).strftime(\"%Y%m%dT235959Z\")\n",
    "            \n",
    "            byday = row['Day']\n",
    "            \n",
    "            event = {\n",
    "                'summary': row['Subject'],\n",
    "                'location': row['Location'],\n",
    "                'description': row['Description'],\n",
    "                'colorId': row['Color ID'],  \n",
    "                'start': {\n",
    "                    'dateTime': start_datetime,\n",
    "                    'timeZone': 'Asia/Manila'\n",
    "                },\n",
    "                'end': {\n",
    "                    'dateTime': end_datetime,\n",
    "                    'timeZone': 'Asia/Manila'\n",
    "                },\n",
    "                'recurrence': [f'RRULE:FREQ=WEEKLY;BYDAY={byday};UNTIL={until_date}'],\n",
    "            }\n",
    "            return event\n",
    "    \n",
    "        current_year = dt.now().year\n",
    "        \n",
    "        for index, row in schedule_df.iterrows():\n",
    "            event = create_weekly_event(row, current_year)\n",
    "            event = service.events().insert(calendarId='primary', body=event).execute()\n",
    "            print(f\"Class {index + 1} of {len(schedule_df)} uploaded: {row['Subject']}\")\n",
    "            \n",
    "    print('\\nEverything is finished uploading! Please check out your calendar!\\n')\n",
    "\n",
    "def sheets_main(df, spreadsheet, sheet):\n",
    "    import os\n",
    "\n",
    "    # Import necessary modules for Google authentication and API access\n",
    "    from google.auth.transport.requests import Request  # For handling HTTP requests during authentication\n",
    "    from google.oauth2.credentials import Credentials  # For managing OAuth 2.0 credentials\n",
    "    from google_auth_oauthlib.flow import InstalledAppFlow  # For handling OAuth 2.0 flow for installed apps\n",
    "    from googleapiclient.discovery import build  # For creating a service object to access the Google API\n",
    "    from googleapiclient.errors import HttpError  # For handling HTTP errors from the API\n",
    "    \n",
    "    sheets_scopes = ['https://www.googleapis.com/auth/spreadsheets']\n",
    "        \n",
    "    creds = None  # Initialize credentials to None\n",
    "\n",
    "    # Check if 'token.json' exists in the current directory\n",
    "    if os.path.exists('sheets_token.json'):\n",
    "        # Load credentials from 'token.json' if it exists\n",
    "        creds = Credentials.from_authorized_user_file('sheets_token.json')\n",
    "\n",
    "    # Check if credentials are invalid or missing\n",
    "    if not creds or not creds.valid:\n",
    "        # If credentials exist but are expired and have a refresh token, refresh them\n",
    "        if creds and creds.expired and creds.refresh_token:\n",
    "            creds.refresh(Request())\n",
    "        else:\n",
    "            # If no valid credentials, start the OAuth flow to get new credentials\n",
    "            flow = InstalledAppFlow.from_client_secrets_file('credentials.json', sheets_scopes)\n",
    "            # Run the local server for OAuth flow and get the credentials\n",
    "            creds = flow.run_local_server(port=0)\n",
    "\n",
    "        # Save the newly acquired credentials to 'token.json' for future use\n",
    "        with open('sheets_token.json', 'w') as token:\n",
    "            token.write(creds.to_json())\n",
    "\n",
    "    service = build('sheets', 'v4', credentials = creds)\n",
    "    sheets = service.spreadsheets()\n",
    "\n",
    "    # Convert the DataFrame to a list of lists\n",
    "    values = [df.columns.tolist()] + df.values.tolist()\n",
    "\n",
    "    # Define the body of the request\n",
    "    body = {\n",
    "        'values': values\n",
    "    }\n",
    "\n",
    "    # Update the sheet with the DataFrame values\n",
    "    result = sheets.values().update(\n",
    "        spreadsheetId=spreadsheet,\n",
    "        range=f'{sheet}!B1',  \n",
    "        valueInputOption='USER_ENTERED',\n",
    "        body=body\n",
    "        ).execute()\n",
    "\n",
    "    print(f\"{result.get('updatedCells')} cells updated in the sheet titled '{sheet}' found in https://docs.google.com/spreadsheets/d/{spreadsheet}!\")\n",
    "\n",
    "'==================================='\n",
    "\n",
    "def get_n_events(number):\n",
    "    import os.path  # to handle file paths\n",
    "    import datetime as dt  # for handling dates and times\n",
    "    import pytz  # Import the pytz library for time zone management\n",
    "    \n",
    "    # Import necessary modules for Google authentication and API access\n",
    "    from google.auth.transport.requests import Request  # For handling HTTP requests during authentication\n",
    "    from google.oauth2.credentials import Credentials  # For managing OAuth 2.0 credentials\n",
    "    from google_auth_oauthlib.flow import InstalledAppFlow  # For handling OAuth 2.0 flow for installed apps\n",
    "    from googleapiclient.discovery import build  # For creating a service object to access the Google API\n",
    "    from googleapiclient.errors import HttpError  # For handling HTTP errors from the API\n",
    "    \n",
    "    Scopes = ['https://www.googleapis.com/auth/calendar']\n",
    "    \n",
    "    \n",
    "    creds = None\n",
    "    if os.path.exists('token.json'):\n",
    "        creds = Credentials.from_authorized_user_file('token.json')\n",
    "    \n",
    "    if not creds or not creds.valid:\n",
    "        if creds and creds.expired and creds.refresh_token:\n",
    "            creds.refresh(Request())\n",
    "        else:\n",
    "            flow = InstalledAppFlow.from_client_secrets_file('credentials.json', Scopes)\n",
    "            creds = flow.run_local_server(port = 0)\n",
    "    \n",
    "        with open('token.json','w') as  token:\n",
    "            token.write(creds.to_json())\n",
    "        \n",
    "    service = build(\"calendar\",'v3', credentials = creds)\n",
    "    philippine_tz = pytz.timezone('Asia/Manila')  # Define the Philippine time zone\n",
    "    now = dt.datetime.now(philippine_tz).isoformat()  # Get the current time in Philippine time zone\n",
    "          \n",
    "    event_result = service.events().list(calendarId = 'primary', timeMin = now, maxResults = number, singleEvents = True, orderBy = 'startTime').execute()\n",
    "    \n",
    "    events = event_result.get('items', [])\n",
    "    \n",
    "    if not events:\n",
    "        print(\"No upcoming events found!\")\n",
    "        return\n",
    "\n",
    "    i = 1\n",
    "    for event in events:\n",
    "        start = event['start'].get('dateTime', event['start'].get('date'))\n",
    "        end = event['end'].get('dateTime', event['end'].get('date'))\n",
    "        \n",
    "        if 'T' in start:\n",
    "            start_dt = dt.datetime.fromisoformat(start)\n",
    "            start = start_dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        if 'T' in end:\n",
    "            end_dt = dt.datetime.fromisoformat(end)\n",
    "            end = end_dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        \n",
    "        print(f\"Event # {i}: {event['summary']} | Starts on: {start} | Ends on: {end} |\")\n",
    "        i += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804d0f88-0173-4f20-9d65-b83c439c440d",
   "metadata": {},
   "source": [
    "### User Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20911582-b780-454f-94ac-cfb27d44753d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tkinter import * # tkinter imports\n",
    "from tkinter import font\n",
    "from tkinter import messagebox\n",
    "from datetime import date\n",
    "\n",
    "import webbrowser # image manipulation imports\n",
    "import subprocess\n",
    "import pyautogui\n",
    "from PIL import Image, ImageEnhance\n",
    "\n",
    "import os\n",
    "\n",
    "import requests\n",
    "import time\n",
    "import threading\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import Select\n",
    "\n",
    "import re\n",
    "import time  \n",
    "import fitz\n",
    "\n",
    "# x------------------------------------------------------------------------------------------------------------------------------------x #\n",
    "# Window Properties\n",
    "tk_root = Tk()\n",
    "tk_root.title(\"Calendar Organizer Dashboard\")\n",
    "tk_root.configure(bg=\"#402E7A\")\n",
    "tk_root.geometry(\"900x600\") # window size can be changed to what looks aesthetically pleasing\n",
    "tk_root.resizable(False, False)\n",
    "\n",
    "# x------------------------------------------------------------------------------------------------------------------------------------x #\n",
    "def main_page():\n",
    "\n",
    "    # x------------------------------------------------------------------------------------------------------------------------------------x #\n",
    "    # everything under the def date_extraction_page contains the code for the different extraction functions\n",
    "    def date_extraction_page():\n",
    "        main_Frame.destroy()\n",
    "\n",
    "        # this function is what the \"BACK\" button in the date_extraction_page executes\n",
    "        def date_extraction_to_main_back():\n",
    "            date_extraction_Frame.destroy()\n",
    "            main_page()\n",
    "\n",
    "        # x------------------------------------------------------------------------------------------------------------------------------------x #\n",
    "        # everything under the def aisis_credentials_class_schedule_page is used to generate the credentials page for the CLASS SCHEDULE SCRAPER\n",
    "        def aisis_credentials_class_schedule_page():\n",
    "            \n",
    "            # removes the container of the date_extraction page, to be replaced by the aisis credentials\n",
    "            date_extraction_Frame.destroy()\n",
    "            \n",
    "            # when called, destroys the container of aisis_credentials_class_schedule_page and recalls the date_extraction_page\n",
    "            def aisis_to_date_extraction_back():\n",
    "                aisis_Frame.destroy()\n",
    "                date_extraction_page()\n",
    "                \n",
    "            # COPY PASTE CLASS SCHEDULE SCRAPER CODE IN THIS FUNCTION   \n",
    "            def class_schedule_scraper(username_class_schedule, password_class_schedule):\n",
    "                sched_df = weekly_sched_extract(username_class_schedule, password_class_schedule)\n",
    "                sheets_main(sched_df, '1HIzSiip1TT7wwGRrLnxLwEnqickJrbc-EunKEmY-z_k', 'Weekly Schedule')\n",
    "                print(\"Please feel free to recheck your schedule's details. You may also customize each class's COLOR ID and DESCRIPTION\")\n",
    "                print('Please do not forget to input the STARTING DATES and ENDING DATES.\\n')\n",
    "            \n",
    "            # start_scraper saves the inputs from tkinter and plugs in the class_schedule_scraper function, placed below because the class_schedule_scraper has to be defined first\n",
    "            def start_scraper():\n",
    "                messagebox.showinfo(title=\"Login Validation\", message=\"Once you press OK, the scraper will open AISIS. Please wait patiently for the scraper.\")\n",
    "                username_class_schedule = username_entry.get()\n",
    "                password_class_schedule = password_entry.get()\n",
    "                threading.Thread(target=class_schedule_scraper, args=(username_class_schedule, password_class_schedule)).start() # this calls the class_schedule_scraper function with the appropriate args\n",
    "            \n",
    "            # x------------------------------------------------------------------------------------------------------------------------------------x #\n",
    "            # everything below this but within the def aisis_credentials page are used for aisis credentials page generator\n",
    "            aisis_Frame = Frame(tk_root, bg=\"#402E7A\")\n",
    "            aisis_Frame.pack()\n",
    "            \n",
    "            # widgets variables\n",
    "            login_label = Label(aisis_Frame, text=\"AISIS Credentials\", bg=\"#402E7A\", fg=\"#FFFFFF\", font=(\"Arial\",30))\n",
    "            username_label = Label(aisis_Frame, text=\"Username\", bg=\"#402E7A\", fg=\"#FFFFFF\", font=(\"Arial\",16))\n",
    "            username_entry = Entry(aisis_Frame)\n",
    "            password_label = Label(aisis_Frame, text=\"Password\", bg=\"#402E7A\", fg=\"#FFFFFF\", font=(\"Arial\",16))\n",
    "            password_entry = Entry(aisis_Frame, show=\"*\")\n",
    "            login_button = Button(aisis_Frame, text=\"Enter\", font=(\"Arial\",16), command = start_scraper)\n",
    "            aisis_back_button = Button(aisis_Frame,text=\"BACK\", command = aisis_to_date_extraction_back)\n",
    "    \n",
    "            # widgets placer\n",
    "            login_label.grid(row=0, column=0, columnspan=2, sticky=\"news\", pady=40)\n",
    "            username_label.grid(row=1, column=0)\n",
    "            username_entry.grid(row=1, column=1, pady=20)\n",
    "            password_label.grid(row=2, column=0)\n",
    "            password_entry.grid(row=2, column=1, pady=20)\n",
    "            login_button.grid(row=3, column=0, columnspan=2, pady=30)\n",
    "            aisis_back_button.grid(row=4, column=0, columnspan=2)\n",
    "            \n",
    "        # x------------------------------------------------------------------------------------------------------------------------------------x #\n",
    "        # everything under the def aisis_credentials_class_schedule_page is used to generate the credentials page for the SYLLABUS DATES SCRAPER\n",
    "        def aisis_credentials_syllabus_dates_page():\n",
    "            \n",
    "            # removes the container of the date_extraction page, to be replaced by the aisis credentials\n",
    "            date_extraction_Frame.destroy()\n",
    "            \n",
    "            # when called, destroys aisis_credentials_syllabus_dates_page recalls the date_extraction_page\n",
    "            def aisis_to_date_extraction_back():\n",
    "                aisis_Frame.destroy()\n",
    "                date_extraction_page()\n",
    "                \n",
    "            # COPY PASTE SYLLABUS DATES SCRAPER CODE IN THIS FUNCTION    \n",
    "            def syllabus_dates_scraper(username_syllabus_dates, password_syllabus_dates):\n",
    "                driver = webdriver.Chrome()\n",
    "                link = r\"https://aisis.ateneo.edu/j_aisis/J_VMCS.do\"\n",
    "    \n",
    "                driver.get(link)\n",
    "                time.sleep(1)\n",
    "                \n",
    "                driver.find_element(By.NAME, 'userName').send_keys(username_syllabus_dates)\n",
    "                driver.find_element(By.NAME, 'password').send_keys(password_syllabus_dates)\n",
    "                \n",
    "                driver.find_element(By.NAME, \"submit\").click()\n",
    "                time.sleep(1)\n",
    "                driver.find_element(By.XPATH, '//a[contains(@href, \"J_VCEC.do\")]').click()\n",
    "               \n",
    "                soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "                time.sleep(1.5)\n",
    "                              \n",
    "                pdf_links = []\n",
    "                \n",
    "                # Assuming the PDF links are in <a> tags and have an href ending in .pdf\n",
    "                for a_tag in soup.find_all('a', href=True):\n",
    "                    pdf_link = a_tag['href']\n",
    "                    if pdf_link.endswith('.pdf'):\n",
    "                        pdf_links.append(pdf_link)\n",
    "                \n",
    "                driver.quit()\n",
    "\n",
    "                client_df = pd.DataFrame(columns = ['Subject', 'Summary', 'Location', 'Date'])\n",
    "                client_df.drop(client_df.index,inplace=True) \n",
    "                total_time = 0\n",
    "                                \n",
    "                for pdf_url in pdf_links:\n",
    "                    if not os.path.exists('pdfs'):\n",
    "                        os.makedirs('pdfs')\n",
    "                    \n",
    "                    response = requests.get(pdf_url)\n",
    "                    pdf_name = os.path.join('pdfs', pdf_url.split('/')[-1])\n",
    "                \n",
    "                    # Check if the PDF already exists\n",
    "                    if os.path.exists(pdf_name):\n",
    "                        print(f\"File already exists: {pdf_name}. Skipping download.\")\n",
    "                    else:   \n",
    "                        with open(pdf_name, 'wb') as pdf_file:\n",
    "                            pdf_file.write(response.content)\n",
    "                        \n",
    "                        print(f'Downloaded PDF: {pdf_name}')\n",
    "                \n",
    "                    text = \"\"\n",
    "                    with fitz.open(pdf_name) as pdf_document:\n",
    "                        for page_num in range(pdf_document.page_count):\n",
    "                            page = pdf_document[page_num]\n",
    "                            text += page.get_text()  \n",
    "                    \n",
    "                    print('Please wait for processing...')\n",
    "                    \n",
    "                    start_time = time.time()  # Record the start time\n",
    "                    \n",
    "                    word_list = word_list_creation(text)\n",
    "                    date_dict = date_find(word_list, text)\n",
    "                    panda_creation(date_dict, word_list, client_df)\n",
    "                      \n",
    "                    end_time = time.time() # Record the end time\n",
    "                    \n",
    "                    pdf_time = round(end_time - start_time, 2)\n",
    "                    \n",
    "                    print(f'File {pdf_links.index(pdf_url)+1} of {len(pdf_links)} processed! Total Time Taken: {pdf_time} seconds\\n')\n",
    "                    \n",
    "                    total_time += pdf_time\n",
    "                    \n",
    "                print(f'Processing Finished! It took {round(total_time, 2)} seconds to process the {len(pdf_links)} documents!\\n')\n",
    "\n",
    "                # This Function copy pastes everything from the pandas DataFrame into a gsheets document\n",
    "                sheets_main(client_df, '1m8badixRPCpTt-dKThNeqW98PK4naJpHsAoUKA_0Uro', 'Syllabus Dates')    \n",
    "                print(\"Please feel free to customize your events' details. You may also customize each event's COLOR ID and DESCRIPTION\\n\")\n",
    "\n",
    "            # start_scraper saves the inputs from tkinter and plugs in the class_schedule_scraper function, placed below because the syllabus_dates_scraper has to be defined first\n",
    "            def start_scraper():\n",
    "                messagebox.showinfo(title=\"Login Validation\", message=\"Once you press OK, the scraper will open AISIS. Please wait patiently for the scraper.\")\n",
    "                username_syllabus_dates = username_entry.get()\n",
    "                password_syllabus_dates = password_entry.get()\n",
    "                threading.Thread(target=syllabus_dates_scraper, args=(username_syllabus_dates, password_syllabus_dates)).start() # this calls the syllabus_dates_scraper function with the appropriate args\n",
    "                  \n",
    "            # everything below this but within the def aisis_credentials page are used for aisis credentials page generator\n",
    "            aisis_Frame = Frame(tk_root, bg=\"#402E7A\")\n",
    "            aisis_Frame.pack()\n",
    "            \n",
    "            # widgets variables\n",
    "            login_label = Label(aisis_Frame, text=\"AISIS Credentials\", bg=\"#402E7A\", fg=\"#FFFFFF\", font=(\"Arial\",30))\n",
    "            username_label = Label(aisis_Frame, text=\"Username\", bg=\"#402E7A\", fg=\"#FFFFFF\", font=(\"Arial\",16))\n",
    "            username_entry = Entry(aisis_Frame)\n",
    "            password_label = Label(aisis_Frame, text=\"Password\", bg=\"#402E7A\", fg=\"#FFFFFF\", font=(\"Arial\",16))\n",
    "            password_entry = Entry(aisis_Frame, show=\"*\")\n",
    "            login_button = Button(aisis_Frame, text=\"Enter\", font=(\"Arial\",16), command = start_scraper)\n",
    "            aisis_back_button = Button(aisis_Frame,text=\"BACK\", command = aisis_to_date_extraction_back)\n",
    "    \n",
    "            # widgets placer\n",
    "            login_label.grid(row=0, column=0, columnspan=2, sticky=\"news\", pady=40)\n",
    "            username_label.grid(row=1, column=0)\n",
    "            username_entry.grid(row=1, column=1, pady=20)\n",
    "            password_label.grid(row=2, column=0)\n",
    "            password_entry.grid(row=2, column=1, pady=20)\n",
    "            login_button.grid(row=3, column=0, columnspan=2, pady=30)\n",
    "            aisis_back_button.grid(row=4, column=0, columnspan=2)\n",
    "        # x------------------------------------------------------------------------------------------------------------------------------------x #\n",
    "        def finalize_dates():\n",
    "            messagebox.showinfo(title=\"WARNING\", message=\"Before clicking OK, make sure to have scraped both of the dates from the syllabus and the class schedule from AISIS.\")\n",
    "\n",
    "            create_events()\n",
    "        \n",
    "        \n",
    "            # *****EXTEND THIS CODE WHEN THE \"\"\"FINALIZE DATES\"\"\" CODE HAS BEEN FINISHED*****\n",
    "\n",
    "        # x------------------------------------------------------------------------------------------------------------------------------------x #\n",
    "        # date_extraction_page container\n",
    "        date_extraction_Frame = Frame(tk_root, bg=\"#402E7A\")\n",
    "        date_extraction_Frame.pack()\n",
    "\n",
    "        # title\n",
    "        date_extraction_title_page = Label(date_extraction_Frame, text=\"Date Extraction\", bg=\"#402E7A\", fg=\"#FFFFFF\", pady=30, font=(\"Arial\", 30))\n",
    "        date_extraction_title_page.grid(row=0, column=0, columnspan=3)\n",
    "        date_extraction_title_page.configure(anchor=\"center\")\n",
    "\n",
    "        # the 4 buttons: \"Scrape Class Schedule\", \"Scrape Syllabus Dates\", \"Finalize Dates\", \"BACK\"\n",
    "        scrape_class_schedule_button = Button(date_extraction_Frame, text=\"Scrape Class Schedule\", command=aisis_credentials_class_schedule_page, font=(\"Arial\", 16), bg=\"#4B70F5\", fg=\"#FFFFFF\")\n",
    "        scrape_class_schedule_button.grid(row=1, column=0, padx=10, pady=20)\n",
    "    \n",
    "        scrape_syllabus_dates_button = Button(date_extraction_Frame, text=\"Scrape Syllabus Dates\", command=aisis_credentials_syllabus_dates_page, font=(\"Arial\", 16), bg=\"#4B70F5\", fg=\"#FFFFFF\")\n",
    "        scrape_syllabus_dates_button.grid(row=1, column=1, padx=10, pady=20)\n",
    "    \n",
    "        calendar_manager_button = Button(date_extraction_Frame, text=\"Finalize Dates\", command=finalize_dates, font=(\"Arial\", 16), bg=\"#4B70F5\", fg=\"#FFFFFF\")\n",
    "        calendar_manager_button.grid(row=1, column=2, padx=10, pady=20)\n",
    "\n",
    "        date_extractor_back_button = Button(date_extraction_Frame, text=\"BACK\", command=date_extraction_to_main_back, font=(\"Arial\", 16), bg=\"#4B70F5\", fg=\"#FFFFFF\")\n",
    "        date_extractor_back_button.grid(row=2, column=1, padx=10, pady=20)\n",
    "        \n",
    "    # x------------------------------------------------------------------------------------------------------------------------------------x #\n",
    "    # for the ClassUp feature\n",
    "    # Note: this is very limited\n",
    "    # 1. The browser has to be Google Chrome\n",
    "    # 2. The program assumes that you're logged in to Google Calendar (Selenium Log In gets blocked)\n",
    "    # 3. The screenshot mechanism takes a screenshot of the entire screen and just crops out the calendar part, so if there's something blocking it, then that's the one that's gonna be screenshotted\n",
    "    # 4. The background image has to be downloaded also\n",
    "    def calendar_generator_page():\n",
    "        # x------------------------------------------------------------------------------------------------------------------------------------x #\n",
    "        # Screenshot Mechanism\n",
    "        url = \"https://calendar.google.com\"\n",
    "        webbrowser.open(url, new=2, autoraise=True)\n",
    "        \n",
    "        # waits for the screen to fully load\n",
    "        time.sleep(5) \n",
    "        \n",
    "        # Take a screenshot of the entire screen\n",
    "        screenshot = pyautogui.screenshot()\n",
    "        screenshot.save(\"full_calendar_image.png\")\n",
    "        \n",
    "        # crop area\n",
    "        crop_area = (300, 210, 1850, 1000)  # make sure that the google chrome is in full screen\n",
    "        \n",
    "        # crops the screenshot\n",
    "        full_calendar_image = Image.open(\"full_calendar_image.png\").convert(\"RGBA\")\n",
    "        cropped_calendar_image = full_calendar_image.crop(crop_area)\n",
    "        cropped_calendar_image.save(\"cropped_calendar_image.png\")\n",
    "        ## x------------------------------------------------------------------------------------------------------------------------------------x #\n",
    "        # Image Manipulation\n",
    "        \n",
    "        # Image Processing 1: makes the foreground transparent before overlaying\n",
    "        image = Image.open(\"cropped_calendar_image.png\").convert(\"RGBA\")\n",
    "        pixels = image.getdata()\n",
    "        new_pixels = []\n",
    "        for item in pixels:\n",
    "            if item[:3] == (255,255,255):\n",
    "                new_pixels.append((255,255,255,0))\n",
    "            else:\n",
    "                new_pixels.append(item)\n",
    "        image.putdata(new_pixels)\n",
    "        image.save(\"cropped_calendar_image_transparent.png\")\n",
    "        \n",
    "        # Image Processing 2 and 3: enhances the images; using the transparent foreground, overlay it with the background\n",
    "        foreground_image = Image.open(\"cropped_calendar_image_transparent.png\").convert(\"RGBA\")\n",
    "        try:\n",
    "            background_image = Image.open(\"background_image.jpg\").convert(\"RGBA\") # assuming that the background_image is saved as \"background_image.jpg\"\n",
    "        except FileNotFoundError:\n",
    "            print(\"Make sure that the background image is downloaded and is saved as 'background_image.jpg'\")\n",
    "        except Exception:\n",
    "            print(\"An error occurred.\")\n",
    "        else:\n",
    "            background_image = background_image.resize(foreground_image.size, Image.LANCZOS)\n",
    "        \n",
    "        # Image Enhancer\n",
    "        background_enhancer = ImageEnhance.Sharpness(background_image)\n",
    "        background_image = background_enhancer.enhance(3)\n",
    "        foreground_enhancer = ImageEnhance.Sharpness(foreground_image)\n",
    "        foreground_image = foreground_enhancer.enhance(3)\n",
    "        \n",
    "        blended_image = Image.alpha_composite(background_image, foreground_image)\n",
    "        blended_image.save(\"final_calendar.png\")\n",
    "        \n",
    "    # x------------------------------------------------------------------------------------------------------------------------------------x #\n",
    "    # for the show N events\n",
    "    def future_events_page():\n",
    "\n",
    "        # destroys the main page in order to show this page\n",
    "        main_Frame.destroy()\n",
    "\n",
    "        def show_future_events(next_events_number):\n",
    "            get_n_events(next_events_number)\n",
    "\n",
    "        def start_future_events():\n",
    "            next_events_number = future_events_number_entry.get()\n",
    "            # you can also convert the data type of next_events_number according to what the code needs, just put it next line\n",
    "            threading.Thread(target=show_future_events, args=(next_events_number,)).start()\n",
    "            # NOTE: args is formatted like that because threading.Thread wants the args to be wrapped in a tuple\n",
    "\n",
    "        # back button\n",
    "        def future_events_to_main_back():\n",
    "            future_events_Frame.destroy()\n",
    "            main_page()\n",
    "            \n",
    "        # container of future_events_page frame\n",
    "        future_events_Frame = Frame(tk_root, bg=\"#402E7A\")\n",
    "        future_events_Frame.pack()\n",
    "\n",
    "        # widgets\n",
    "        future_events_title = Label(future_events_Frame, text=\"Future Events\", bg=\"#402E7A\", fg=\"#FFFFFF\", font=(\"Arial\",30))\n",
    "        future_events_number_label = Label(future_events_Frame, text=\"Enter number of events you want to see next: \", bg=\"#402E7A\", fg=\"#FFFFFF\", font=(\"Arial\",16))\n",
    "        future_events_number_entry = Entry(future_events_Frame)\n",
    "        future_events_enter_button = Button(future_events_Frame, text=\"Enter\", font=(\"Arial\",16), command = start_future_events)\n",
    "        future_events_back_button = Button(future_events_Frame,text=\"BACK\", command = future_events_to_main_back)\n",
    "\n",
    "        # widgets placer\n",
    "        future_events_title.grid(row=0, column=0, columnspan=2, sticky=\"news\", pady=40)\n",
    "        future_events_number_label.grid(row=1, column=0)\n",
    "        future_events_number_entry.grid(row=1, column=1, pady=20)\n",
    "        future_events_enter_button.grid(row=2, column=0, columnspan=2, pady=30)\n",
    "        future_events_back_button.grid(row=3, column=0, columnspan=2)\n",
    "        \n",
    "    # x------------------------------------------------------------------------------------------------------------------------------------x #\n",
    "    # insert ring chart\n",
    "    def today_events_page():\n",
    "        main_Frame.destroy()\n",
    "        \n",
    "        from google.oauth2.credentials import Credentials\n",
    "        from googleapiclient.discovery import build\n",
    "        import pytz\n",
    "        import datetime as dt\n",
    "        from collections import Counter\n",
    "        import matplotlib.pyplot as plt\n",
    "        from matplotlib.backends.backend_tkagg import FigureCanvasTkAgg\n",
    "\n",
    "        def get_events_today():\n",
    "            # credentials to access google calendar\n",
    "            SCOPES = ['https://www.googleapis.com/auth/calendar.readonly']  # Restore the .readonly scope\n",
    "            TIMEZONE = 'Asia/Singapore'  # GMT+08:00\n",
    "\n",
    "            creds = None\n",
    "            if os.path.exists('token.json'):\n",
    "                creds = Credentials.from_authorized_user_file('token.json', SCOPES)\n",
    "            if not creds or not creds.valid:\n",
    "                if creds and creds.expired and creds.refresh_token:\n",
    "                    creds.refresh(Request())\n",
    "                else:\n",
    "                    flow = InstalledAppFlow.from_client_secrets_file('credentials.json', SCOPES)\n",
    "                    creds = flow.run_local_server(port=0)\n",
    "                with open('token.json', 'w') as token:\n",
    "                    token.write(creds.to_json())\n",
    "        \n",
    "            service = build('calendar', 'v3', credentials=creds)\n",
    "        \n",
    "            tz = pytz.timezone(TIMEZONE)  # converts the time to Singapore Time\n",
    "            now = dt.datetime.now(tz).isoformat()\n",
    "            end_of_day = (dt.datetime.now(tz) + dt.timedelta(days=1)).replace(\n",
    "                hour=0, minute=0, second=0, microsecond=0).isoformat()\n",
    "        \n",
    "            # gets the list of the events on the day\n",
    "            events_result = service.events().list(calendarId='primary', timeMin=now, timeMax=end_of_day,\n",
    "                                                  maxResults=100, singleEvents=True,\n",
    "                                                  orderBy='startTime').execute()\n",
    "            events = events_result.get('items', [])\n",
    "            return events\n",
    "        \n",
    "        def ring_chart_schedule(event_counts):\n",
    "            labels = list(event_counts.keys())  # list of name of events\n",
    "            sizes = list(event_counts.values())  # list of the sizes of events compared to the day\n",
    "            colors = plt.cm.Paired(range(len(labels)))  # just uses matplotlib colors depending on how many events there are\n",
    "        \n",
    "            fig, ax = plt.subplots()\n",
    "            ax.pie(sizes, labels=labels, colors=colors, startangle=90, wedgeprops=dict(width=0.3))\n",
    "        \n",
    "            centre_circle = plt.Circle((0, 0), 0.70, fc='white')\n",
    "            fig.gca().add_artist(centre_circle)\n",
    "        \n",
    "            ax.axis('equal')\n",
    "            plt.title(\"Distribution of Today's Events\")\n",
    "            return fig\n",
    "        \n",
    "        def update_chart():\n",
    "            events = get_events_today()\n",
    "            event_counts = Counter(event['summary'] for event in events)\n",
    "            fig = ring_chart_schedule(event_counts)\n",
    "        \n",
    "            for widget in today_events_Frame.winfo_children():  # when a new chart is created, it destroys all the old ones first before placing the new one\n",
    "                widget.destroy()\n",
    "        \n",
    "            canvas = FigureCanvasTkAgg(fig, master=today_events_Frame)\n",
    "            canvas.draw()\n",
    "            canvas.get_tk_widget().pack(fill=BOTH, expand=True)\n",
    "        \n",
    "        # container of the chart\n",
    "        today_events_Frame = Frame(tk_root)\n",
    "        today_events_Frame.pack(fill=BOTH, expand=True)\n",
    "        \n",
    "        # Call the update_chart function to initialize the chart\n",
    "        update_chart()\n",
    "    # x------------------------------------------------------------------------------------------------------------------------------------x #\n",
    "        \n",
    "    # main_page container\n",
    "    main_Frame = Frame(tk_root, bg=\"#402E7A\")\n",
    "    main_Frame.pack()\n",
    "\n",
    "    # title\n",
    "    title_page = Label(main_Frame, text=\"Calendar Organizer Dashboard\", bg=\"#402E7A\", fg=\"#FFFFFF\", pady=30, font=(\"Arial\", 30))\n",
    "    title_page.grid(row=0, column=0, columnspan=3)\n",
    "    title_page.configure(anchor=\"center\")\n",
    "    \n",
    "    # the 3 buttons in the main page: \"Date Extraction\", \"Calendar Generator\", \"Calendar Manager\"\n",
    "    date_extraction_button = Button(main_Frame, text=\"Date Extraction\", command=date_extraction_page, font=(\"Arial\", 16), bg=\"#4B70F5\", fg=\"#FFFFFF\")\n",
    "    date_extraction_button.grid(row=1, column=0, padx=10, pady=20)\n",
    "\n",
    "    calendar_generator_button = Button(main_Frame, text=\"Calendar Generator\", command=calendar_generator_page, font=(\"Arial\", 16), bg=\"#4B70F5\", fg=\"#FFFFFF\")\n",
    "    calendar_generator_button.grid(row=1, column=1, padx=10, pady=20)\n",
    "\n",
    "    future_events_button = Button(main_Frame, text=\"Future Events\", command=future_events_page, font=(\"Arial\", 16), bg=\"#4B70F5\", fg=\"#FFFFFF\")\n",
    "    future_events_button.grid(row=1, column=2, padx=10, pady=20)\n",
    "    \n",
    "    # formatting and showing the date today\n",
    "    today = date.today()\n",
    "    show_day_today = Label(main_Frame, text=\"Today is: \" + today.strftime('%B %d, %Y') + \". Click button to see today's tasks: \", bg=\"#402E7A\", fg=\"#FFFFFF\", pady=30, font=(\"Arial\", 20))\n",
    "    show_day_today.grid(row=2, column=0, columnspan=3)\n",
    "    show_day_today.configure(anchor=\"center\")\n",
    "\n",
    "    # the 4th button in the main page: \"Today's Events\"\n",
    "    today_events_button = Button(main_Frame, text=\"Today's Events\", command=today_events_page, font=(\"Arial\", 16), bg=\"#4B70F5\", fg=\"#FFFFFF\")\n",
    "    today_events_button.grid(row=3, column=1, padx=10, pady=20)\n",
    "    \n",
    "# x------------------------------------------------------------------------------------------------------------------------------------x #\n",
    "# calls the main_page as the beginning\n",
    "main_page()\n",
    "tk_root.mainloop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
